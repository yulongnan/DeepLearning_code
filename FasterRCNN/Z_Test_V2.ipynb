{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 训练数据与验证数据分开读取\r\n",
    "## 数据集的划分"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import time \r\n",
    "\r\n",
    "import cv2 \r\n",
    "import numpy as np \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# 训练集 读取\r\n",
    "train_annotation_path = '2007_train.txt'\r\n",
    "with open(train_annotation_path) as f:  \r\n",
    "    train_lines = f.readlines()  \r\n",
    "np.random.seed(10101) \r\n",
    "np.random.shuffle(train_lines)  \r\n",
    "np.random.seed(None) \r\n",
    "num_train = len(train_lines)  # 获取训练集数量\r\n",
    "\r\n",
    "#验证集 读取\r\n",
    "val_annotation_path = '2007_val.txt'\r\n",
    "with open(val_annotation_path) as f:\r\n",
    "        val_lines = f.readlines() \r\n",
    "np.random.seed(10101) \r\n",
    "np.random.shuffle(val_lines)  \r\n",
    "np.random.seed(None)\r\n",
    "num_val=len(val_lines)       # 获取验证集数量 \r\n",
    "\r\n",
    "print('train_lines: \\r\\n', train_lines) \r\n",
    "print('val_lines: \\r\\n', val_lines) \r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_lines: \n",
      " ['F:\\\\NYL_FWorkFile\\\\NYL_Trian\\\\faster-rcnn-pytorch-master_Rb_X1/VOCdevkit/VOC2007/JPEGImages/OriRGB_pitaya_000015.jpg 567,461,698,574,0 607,416,699,501,0 554,1101,681,1203,0\\n', 'F:\\\\NYL_FWorkFile\\\\NYL_Trian\\\\faster-rcnn-pytorch-master_Rb_X1/VOCdevkit/VOC2007/JPEGImages/OriRGB_pitaya_000002.jpg 180,408,337,534,0\\n', 'F:\\\\NYL_FWorkFile\\\\NYL_Trian\\\\faster-rcnn-pytorch-master_Rb_X1/VOCdevkit/VOC2007/JPEGImages/OriRGB_pitaya_000011.jpg 629,870,695,969,0\\n', 'F:\\\\NYL_FWorkFile\\\\NYL_Trian\\\\faster-rcnn-pytorch-master_Rb_X1/VOCdevkit/VOC2007/JPEGImages/OriRGB_pitaya_000001.jpg 274,832,404,927,0 479,830,584,946,0 403,847,480,959,0\\n', 'F:\\\\NYL_FWorkFile\\\\NYL_Trian\\\\faster-rcnn-pytorch-master_Rb_X1/VOCdevkit/VOC2007/JPEGImages/OriRGB_pitaya_000003.jpg 298,78,418,215,0\\n', 'F:\\\\NYL_FWorkFile\\\\NYL_Trian\\\\faster-rcnn-pytorch-master_Rb_X1/VOCdevkit/VOC2007/JPEGImages/OriRGB_pitaya_000014.jpg 295,443,434,558,0 340,400,440,489,0 248,1085,365,1177,0 385,874,438,966,0 488,938,600,1059,0 485,1068,556,1162,0\\n', 'F:\\\\NYL_FWorkFile\\\\NYL_Trian\\\\faster-rcnn-pytorch-master_Rb_X1/VOCdevkit/VOC2007/JPEGImages/OriRGB_pitaya_000008.jpg 275,834,373,931,0 572,714,661,792,0 533,1031,649,1105,0 519,1195,644,1280,0\\n', 'F:\\\\NYL_FWorkFile\\\\NYL_Trian\\\\faster-rcnn-pytorch-master_Rb_X1/VOCdevkit/VOC2007/JPEGImages/OriRGB_pitaya_000007.jpg 183,832,283,936,0 472,700,552,782,0 641,669,720,746,0 434,1016,553,1093,0 423,1173,548,1280,0\\n', 'F:\\\\NYL_FWorkFile\\\\NYL_Trian\\\\faster-rcnn-pytorch-master_Rb_X1/VOCdevkit/VOC2007/JPEGImages/OriRGB_pitaya_000005.jpg 502,947,602,1073,0 141,744,204,821,0\\n', 'F:\\\\NYL_FWorkFile\\\\NYL_Trian\\\\faster-rcnn-pytorch-master_Rb_X1/VOCdevkit/VOC2007/JPEGImages/OriRGB_pitaya_000004.jpg 543,155,652,242,0\\n']\n",
      "val_lines: \n",
      " ['F:\\\\NYL_FWorkFile\\\\NYL_Trian\\\\faster-rcnn-pytorch-master_Rb_X1/VOCdevkit/VOC2007/JPEGImages/OriRGB_pitaya_000006.jpg 6,603,126,703,0 173,968,306,1139,0\\n', 'F:\\\\NYL_FWorkFile\\\\NYL_Trian\\\\faster-rcnn-pytorch-master_Rb_X1/VOCdevkit/VOC2007/JPEGImages/OriRGB_pitaya_000012.jpg 354,538,461,626,0 296,861,367,964,0 614,670,720,768,0\\n']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "Tlist = np.arange(1,100,1)\r\n",
    "print(Tlist)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "import random \r\n",
    "num = len(Tlist) \r\n",
    "random.sample "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<bound method Random.sample of <random.Random object at 0x000001CA799A1A68>>"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "np.random.seed(0) #随机种子0，随机数固定 \r\n",
    "\r\n",
    "train_percent = 0.7 \r\n",
    "val_percent = 0.15 \r\n",
    "test_percent = 0.15 \r\n",
    "\r\n",
    "num_Total =100\r\n",
    "\r\n",
    "num_train = int(train_percent * num_Total)\r\n",
    "num_val = int(val_percent * num_Total)\r\n",
    "num_test = int(test_percent * num_Total)\r\n",
    "\r\n",
    "\r\n",
    "datasetindex = np.arange(1, num_Total+1, 1) \r\n",
    "print('datasetindex',datasetindex, len(datasetindex ))  \r\n",
    "\r\n",
    "train_index = np.random.choice(datasetindex, num_train, replace = False)   # replace = False 保证元素不重复\r\n",
    "print('train_index',np.sort(train_index), len(train_index))   \r\n",
    "\r\n",
    "datasetindex_remian= list(set(datasetindex) ^ set(train_index)) #剩余的数据集索引   \r\n",
    "print('datasetindex_remian',datasetindex_remian, len(datasetindex_remian))   \r\n",
    "\r\n",
    "val_index = np.random.choice(datasetindex_remian, num_val, replace = False)   # replace = False 保证元素不重复\r\n",
    "print('val_index', np.sort(val_index), len(val_index),type(val_index))  \r\n",
    "\r\n",
    "test_index = list( set(datasetindex_remian) ^ set(val_index) ) \r\n",
    "print('test_index', np.sort(test_index),len(test_index) )   \r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "datasetindex [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100] 100\n",
      "train_index [  1   2   3   4   5   6   7   8   9  11  12  14  15  16  17  18  19  23\n",
      "  24  25  27  28  29  31  32  33  34  35  36  39  41  42  43  44  46  49\n",
      "  51  52  53  54  55  56  57  58  60  61  62  63  64  67  69  72  74  75\n",
      "  76  77  79  80  81  83  85  86  87  90  91  92  93  94  96 100] 70\n",
      "datasetindex_remian [10, 13, 20, 21, 22, 26, 30, 37, 38, 40, 45, 47, 48, 50, 59, 65, 66, 68, 70, 71, 73, 78, 82, 84, 88, 89, 95, 97, 98, 99] 30\n",
      "val_index [13 22 26 30 38 48 59 65 68 70 71 82 84 88 97] 15 <class 'numpy.ndarray'>\n",
      "test_index [10 20 21 37 40 45 47 50 66 73 78 89 95 98 99] 15\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "classes = [\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\r\n",
    "len(classes)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "import os\r\n",
    "def Get_weightFile():\r\n",
    "    import os\r\n",
    "    files = os.listdir('./logs')  \r\n",
    "    file_weight_pth = []\r\n",
    "    for file in files:\r\n",
    "        if os.path.splitext(file)[1]=='.pth':\r\n",
    "            file_weight_pth.append(file)\r\n",
    "    file_weight_pth.sort(key=lambda ele:ele[5:8], reverse=True)\r\n",
    "    if len(file_weight_pth) == 0:\r\n",
    "        return print('no Get_weightFile')\r\n",
    "    else:\r\n",
    "        return file_weight_pth[0]\r\n",
    "\r\n",
    "Get_weightFile()\r\n",
    "\r\n",
    "model_path = os.path.join('logs/', Get_weightFile())\r\n",
    "print(model_path)\r\n",
    "epoch_Cur = int(Get_weightFile()[5:8])\r\n",
    "print(epoch_Cur)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "logs/Epoch055-Total_Loss1.0760-Val_Loss0.9385.pth\n",
      "55\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# 获得训练中断标记\r\n",
    "def Get_InterruptFlag_FindweightFile():\r\n",
    "    import os\r\n",
    "    files = os.listdir('./logs')  \r\n",
    "    file_weight_pth = []\r\n",
    "    for file in files:\r\n",
    "        if os.path.splitext(file)[1]=='.pth':\r\n",
    "            file_weight_pth.append(file)\r\n",
    "    file_weight_pth.sort(key=lambda ele:ele[5:8], reverse=True)\r\n",
    "    if len(file_weight_pth) == 0:  #无'pth'文件 = 无中断\r\n",
    "        return False             \r\n",
    "    else:\r\n",
    "        return True           #有'pth'文件 = 有中断\r\n",
    "\r\n",
    "Get_InterruptFlag_FindweightFile()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "files = os.listdir('./logs')  \r\n",
    "for file in files:\r\n",
    "    if file[:5] == 'loss_':\r\n",
    "        time_str = file[5:]\r\n",
    "        print(time_str)\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021_08_31_19_50_03\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# 读取txt文件\r\n",
    "import pandas as pd\r\n",
    "df =  pd.read_table(os.path.join( './logs','loss_'+ time_str, 'epoch_loss_'+ time_str+'.txt'),header=None)\r\n",
    "print(df)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                        0\n",
      "0  1.6573744018872578,1.8\n",
      "1  1.6004694700241089,1.9\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "rootpath = os.path.join( './logs','loss_'+ time_str, 'epoch_loss_'+ time_str+'.txt')\r\n",
    "list(np.loadtxt(rootpath , delimiter=',', unpack=True ) )\r\n",
    "# list( np.loadtxt(os.path.join( './logs','loss_'+ time_str, 'epoch_val_loss_'+ time_str+'.txt')) )\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[array([1.6573744 , 1.60046947]), array([1.8, 1.9])]"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "epoch = 5\r\n",
    "\r\n",
    "with open('./logs/NYL_Test.txt', 'a+') as f:\r\n",
    "            f.write('epoch'+str(epoch).zfill(3) + ','+ str(0.555) )\r\n",
    "            f.write(\"\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "skiprows=1, \r\n",
    "np.loadtxt('./logs/NYL_Test.txt' , delimiter=',',usecols=(1))\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.555, 0.555, 0.555, 0.555, 0.555, 0.555, 0.555, 0.555, 0.555,\n",
       "       0.555, 0.555, 0.555, 0.555, 0.555, 0.555, 0.555, 0.555])"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "\r\n",
    "rootpath = os.path.join( './logs','loss_'+ time_str, 'epoch_loss_'+ time_str+'.txt')\r\n",
    "np.loadtxt(rootpath , delimiter=',',usecols=(1))\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1.8, 1.9])"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "\r\n",
    "rootpath2 = os.path.join( './logs','loss_'+ time_str, 'epoch_val_loss_'+ time_str+'.txt')\r\n",
    "L=list(np.loadtxt(rootpath2 , delimiter=',',usecols=(1)))\r\n",
    "L.append(0.5)\r\n",
    "L\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1.9, 1.9, 0.5]"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# 读取txt文件\r\n",
    "import pandas as pd\r\n",
    "Txtpd = pd.read_table('./logs/NYL_Test.txt' , delimiter=',',header=None ) \r\n",
    "np.array(Txtpd)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([['epoch005', 0.555],\n",
       "       ['epoch005', 0.555],\n",
       "       ['epoch005', 0.555],\n",
       "       ['epoch005', 0.555],\n",
       "       ['epoch005', 0.555],\n",
       "       ['epoch005', 0.555],\n",
       "       ['epoch005', 0.555],\n",
       "       ['epoch005', 0.555],\n",
       "       ['epoch005', 0.555],\n",
       "       ['epoch005', 0.555],\n",
       "       ['epoch005', 0.555],\n",
       "       ['epoch005', 0.555],\n",
       "       ['epoch005', 0.555],\n",
       "       ['epoch005', 0.555],\n",
       "       ['epoch005', 0.555],\n",
       "       ['epoch005', 0.555],\n",
       "       ['epoch005', 0.555]], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "Flag = True\r\n",
    "if not Flag:\r\n",
    "    print('1')\r\n",
    "else:\r\n",
    "    print('0')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}